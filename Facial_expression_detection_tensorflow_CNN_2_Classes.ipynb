{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facial expression detection - tensorflow-CNN-2 Classes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXQA5IhyMt9R"
      },
      "source": [
        "### Tensorflow with Convolution Layers Example - 2 classes facial expression detection sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9qR3suBLfJd"
      },
      "source": [
        "#Python v3 \n",
        "#Tensorflow with Convolution Layers Example - 2 classes facial expression detection sample\n",
        "# Colab code - Adapted From Coursera  - DeepLearning.AI TensorFlow CNN course \n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "\n",
        "local_zip = '/tmp/facial_expressions.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/facial_expressions')\n",
        "local_zip = '/tmp/facial_expressions_validation.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/facial_expressions_validation')\n",
        "zip_ref.close()\n",
        "\n",
        "train_happy_dir = os.path.join('/tmp/facial_expressions/happy')\n",
        "train_sad_dir = os.path.join('/tmp/facial_expressions/sad')\n",
        "\n",
        "validation_happy_dir = os.path.join('/tmp/facial_expressions_validation/happy')\n",
        "validation_sad_dir = os.path.join('/tmp/facial_expressions_validation/sad')\n",
        "\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/facial_expressions/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/facial_expressions_validation/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=32,\n",
        "\n",
        "        class_mode='binary')\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=1,  \n",
        "      epochs=50,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=1)\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "#print(train_generator.class_indices)\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  \n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is sad\")\n",
        "  else:\n",
        "    print(fn + \" is happy\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}